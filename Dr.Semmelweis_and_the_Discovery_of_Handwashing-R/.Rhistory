order(a)
a <- c(100, 10, 1000)
order("a")
a <- c(100, 10, 1000)
order(1:3)
a <- c(100, 10, 1000)
order(1:3)
order("a")
order(a)
order[a]
order["a"]
a[order(1:3)]
a <- c(100, 10, 1000)
a[order(1:3)]
a[order(a)]
y <- c(0, 11, 18, -19, -2, 3)
y[2:4]
character_vector <- f("python", "r", "sql")
character_vector <- c("python", "r", "sql")
# Chapter 4 - Decision trees
# 3.1 - Building a simple decision tree
loans <- read_csv("https://assets.datacamp.com/production/course_2906/datasets/loans.csv")
library(tidyverse)
loans <- read_csv("https://assets.datacamp.com/production/course_2906/datasets/loans.csv")
# Load the rpart package
library(rpart)
# Build a lending model predicting loan outcome versus loan amount and credit score
loan_model <- rpart(outcome ~ loan_amount+credit_score, data = loans, method = "class", control = rpart.control(cp = 0))
# Make a prediction for someone with good credit
predict(loan_model, good_credit, type = "class")
# Make a prediction for someone with bad credit
predict(loan_model, bad_credit, type = "class")
# Make a prediction for someone with bad credit
predict(loan_model, bad_credit, type = "class")
loans_revised <- loans %>%
filter(keep == 1) %>%
select(-rand) %>%
mutate(outcome = if_else(default == 0, "repaid","default"))
loans_clean <- loans_revised %>%
select(-keep, -default)
loans <- loans_clean
# Build a lending model predicting loan outcome versus loan amount and credit score
loan_model <- rpart(outcome ~ loan_amount + credit_score, data = loans, method = "class", control = rpart.control(cp = 0))
# Make a prediction for someone with good credit
predict(loan_model, good_credit, type = "class")
good_credit <- slice(loans, 8)
bad_credit <- slice(loans, 3)
# Make a prediction for someone with good credit
predict(loan_model, good_credit, type = "class")
# Make a prediction for someone with bad credit
predict(loan_model, bad_credit, type = "class")
# Load the rpart.plot package
library(rpart.plot)
# Plot the loan_model with default settings
rpart.plot(loan_model)
# Plot the loan_model with customized settings
rpart.plot(loan_model, type = 3, box.palette = c("red", "green"), fallen.leaves = TRUE)
# Examine the loan_model object
loan_model
# Determine the number of rows for training
nrows(loan)
nrow(loan)
nrow(loans)
# Create a random sample of row IDs
sample_rows <- sample(id, 0.75)
sample_rows <- sample(nrows(loans), 0.75)
sample_rows <- sample(nrow(loans), 0.75)
nrow(loans*0.75)
# Determine the number of rows for training
a <- 0.75* nrow(loans)
b <- nrow(loans)
# Create a random sample of row IDs
sample_rows <- sample(b, a)
# Create the training dataset
loans_train <- loans[sample_rows,]
# Create the test dataset
loans_test <- loans[-sample_rows,]
# Grow a tree using all of the available applicant data
loan_model <- rpart(outcome ~ . , data = loans_train, method = "class", control = rpart.control(cp = 0))
# Make predictions on the test dataset
loans_test$pred <- predict(loan_model, loans_test, method="class")
# Examine the confusion matrix
table(loans_test$outcome, loans_test$pred)
# Compute the accuracy on the test dataset
mean(loans_test$outcome == loans_test$pred)
View(loan_model)
View(loans_test)
View(loans_test)
# Make predictions on the test dataset
loans_test$pred <- predict(loan_model, loans_test, type="class")
# Examine the confusion matrix
table(loans_test$outcome, loans_test$pred)
# Compute the accuracy on the test dataset
mean(loans_test$outcome == loans_test$pred)
# Swap maxdepth for a minimum split of 500
loan_model <- rpart(outcome ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0, minsplit = 500))
# Run this. How does the accuracy change?
loans_test$pred <- predict(loan_model, loans_test, type = "class")
mean(loans_test$pred == loans_test$outcome)
# Grow a tree using all of the available applicant data
loan_model <- rpart(outcome ~ . , data = loans_train, method = "class", control = rpart.control(cp = 0))
# Make predictions on the test dataset
loans_test$pred <- predict(loan_model, loans_test, type="class")
# Examine the confusion matrix
table(loans_test$outcome, loans_test$pred)
# Compute the accuracy on the test dataset
mean(loans_test$outcome == loans_test$pred)
#4.5 Preventing overgrown trees
# Grow a tree with maxdepth of 6
loan_model <- rpart(outcome ~ . , data = loans_train, method = "class", control = rpart.control(cp = 0, maxdepth = 6))
# Make a class prediction on the test set
loans_test$pred <- predict(loan_model, loans_test, type="class")
# Compute the accuracy of the simpler tree
mean(loans_test$outcome == loans_test$pred)
# Remove maxdepth and replace with minsplit = 500
# Swap maxdepth for a minimum split of 500
loan_model <- rpart(outcome ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0, minsplit = 500))
# Run this. How does the accuracy change?
loans_test$pred <- predict(loan_model, loans_test, type = "class")
mean(loans_test$pred == loans_test$outcome)
# Grow an overly complex tree
loan_model <-  rpart(outcome ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0))
# Examine the complexity plot
plotcp(loan_model)
# Prune the tree
loan_model_pruned <- prune(loan_model, cp = 0.0014)
# Compute the accuracy of the pruned tree
loans_test$pred <- predict(loan_model_pruned, loans_test, type = "class")
mean(loans_test$pred == loans_test$outcome)
# Load the randomForest package
load(randomForest)
# Build a random forest model
loan_model <- randomForest(outcome ~., data = loans)
install.packages("randomForest", dependencies = FALSE)
library(randomForest)
# Build a random forest model
loan_model <- randomForest(outcome ~., data = loans)
# Compute the accuracy of the random forest
loans_test$pred <- predict(loan_model, loans_test, type = "class")
mean(loans_test$pred == loans_test$outcome)
# Import the csv file: pools
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv"
# Import the txt file: potatoes
url_delim <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt"
print(url_csv)
print(url_delim)
url_delim
url_csv
url_delim
# Import the csv file: pools
url_csv <- read_csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv")
# Import the txt file: potatoes
url_delim <- read_tsv("http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt")
url_csv
url_delim
library(readr)
# Import the csv file: pools
url_csv <- read_csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv")
# Import the txt file: potatoes
url_delim <- read_tsv("http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt")
url_csv
url_delim
url_csv <- "https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv"
# Import the file using read.csv(): pools1
pools1 <- read.csv(url_csv)
# Load the readr package
library(readr)
# Import the file using read_csv(): pools2
pools2 <- read_csv(url_csv)
# Print the structure of pools1 and pools2
pools1
pools2
# Load the readr package
url_csv <- "https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv"
# Import the file using read.csv(): pools1
pools1 <- read.csv(url_csv)
# Load the readr package
library(readr)
# Import the file using read_csv(): pools2
pools2 <- read_csv(url_csv)
# Print the structure of pools1 and pools2
str(pools1)
str(pools2)
library(readxl)
library(gdata)
# Specification of url: url_xls
url_xls <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls"
# Import the .xls file with gdata: excel_gdata
excel_gdata <- read.xls(url_xls)
# Download file behind URL, name it local_latitude.xls
download.file(url_xls, "local_latitude.xls")
# Import the local .xls file with readxl: excel_readxl
excel_readxl= read_excel("local_latitude.xls")
install.pacakages ('rmarkdown')
install.packages("rmarkdown", dependencies = FALSE)
The `investment_services_projects` dataset provides information about each investment project from the 2012 to 2018 fiscal years. Information listed includes the project name, company name, sector, project status, and investment amounts.
```{r}
investment_services_projects
```
install.packages("bslib", dependencies = FALSE)
install.packages("htmltools", dependencies = FALSE)
install.packages("tinytex", dependencies = FALSE)
install.packages("rmarkdown")
install.packages(c("babynames", "backports", "benchmarkme", "BH", "blob", "brew", "broom", "C50", "cachem", "callr", "car", "carData", "caret", "checkmate", "chron", "class", "cli", "clipr", "clock", "colorspace", "commonmark", "corrplot", "covr", "crayon", "crosstalk", "Cubist", "curl", "data.table", "DBI", "dbplyr", "dbscan", "dendextend", "desc", "devtools", "digest", "doParallel", "dplyr", "DT", "evaluate", "extrafont", "fansi", "faraway", "farver", "forcats", "foreach", "forecast", "Formula", "fracdiff", "fs", "furrr", "future", "gbm", "gdata", "GGally", "ggdendro", "ggfittext", "ggforce", "ggformula", "ggplot2", "ggpubr", "ggrepel", "ggsignif", "ggstance", "gh", "git2r", "globals", "glue", "gmp", "goftest", "gower", "gplots", "gtable", "gtools", "h2o", "haven", "HH", "highr", "Hmisc", "hms", "htmlTable", "htmltools", "htmlwidgets", "httpuv", "httr", "hunspell", "igraph", "imager", "infer", "inspectdf", "ipred", "ISLR", "isoband", "iterators", "janeaustenr", "jpeg", "jsonlite", "
kernlab", "knitr", "labeling", "later", "latticeExtra", "lava", "leaflet", "lhs", "libcoin", "lifecycle", "listenv", "lme4", "lmtest", "lubridate", "magrittr", "maptools", "markdown", "MASS", "MatrixModels", "matrixStats", "mdsr", "mime", "minqa", "misc3d", "mlbench", "mockery", "modelr", "mosaic", "mosaicCore", "mosaicData", "multcomp", "mvtnorm", "nloptr", "norm", "openair", "openssl", "openxlsx", "packrat", "padr", "parallelly", "partykit", "paws.application.integration", "paws.common", "paws.compute", "pbkrtest", "pillar", "pkgbuild", "pkgload", "plot3D", "plyr", "png", "polyclip", "polynom", "prim", "pROC", "processx", "promises", "proxy", "ps", "purrr", "quantmod", "quantreg", "R6", "ranger", "raster", "rattle", "rcmdcheck", "Rcpp", "RcppArmadillo", "RcppEigen", "RcppParallel", "RcppTOML", "RCurl", "reactable", "readr", "readxl", "remotes", "repr", "reprex", "reshape", "reshape2", "reticulate", "rex", "rio", "rJava", "rlang", "Rlof", "Rmpfr", "RMySQL", "roxygen2", "rpart", "rpar
t.plot", "rsample", "rsconnect", "rstan", "rstatix", "Rttf2pt1", "rversions", "rvest", "sandwich", "sass", "scales", "sessioninfo", "shiny", "slam", "slider", "sp", "SparseM", "SQUAREM", "statmod", "stringi", "stringr", "survival", "sys", "testthat", "TH.data", "tibble", "tibbletime", "tidyr", "tidyselect", "tidytext", "tidyverse", "tiff", "timechange", "timeDate", "tinytex", "tm", "tokenizers", "tree", "tseries", "TTR", "tweenr", "tzdb", "urca", "usethis", "utf8", "V8", "vcd", "vctrs", "viridis", "viridisLite", "whisker", "xfun", "xgboost", "xml2", "xts", "yaml", "yardstick", "zip", "zoo"))
install.packages("tidyverse", dependencies = FALSE)
install.packages("googledrive", dependencies = FALSE)
install.packages("gargle", dependencies = FALSE)
library(markdown)
library(markdown)
install.packages("https://cran.r-project.org/bin/macosx/contrib/4.0/htmltools_0.4.0.tgz", repos = NULL, type="source")
install.packages("htmltools")
help(htmltools)in
install.packages("bslib", dependencies = FALSE)
install.packages("bslib", dependencies = FALSE)
install.packages("rchess")
install.packages("pryr", dependencies = FALSE)
# Data Cleaning in R
# Common Data Problems
# Data types
# Converting data types
# We'll be working with San Francisco bike share ride data called bike_share_rides.
# It contains information on start and end stations of each trip, the trip duration,
# and some user information.
# Before beginning to analyze any dataset, it's important to take a look at the
# different types of columns you'll be working with, which you can do using glimpse().
library(dplyr)
library(assertive)
library(stringr)
library(ggplot2)
install.packages("rlang", dependencies = FALSE)
# Data Cleaning in R
# Common Data Problems
# Data types
# Converting data types
# We'll be working with San Francisco bike share ride data called bike_share_rides.
# It contains information on start and end stations of each trip, the trip duration,
# and some user information.
# Before beginning to analyze any dataset, it's important to take a look at the
# different types of columns you'll be working with, which you can do using glimpse().
library(dplyr)
library(assertive)
library(stringr)
library(ggplot2)
install.packages("rlang")
# Data Cleaning in R
# Common Data Problems
# Data types
# Converting data types
# We'll be working with San Francisco bike share ride data called bike_share_rides.
# It contains information on start and end stations of each trip, the trip duration,
# and some user information.
# Before beginning to analyze any dataset, it's important to take a look at the
# different types of columns you'll be working with, which you can do using glimpse().
library(dplyr)
library(assertive)
library(stringr)
library(ggplot2)
# Data Cleaning in R
# Common Data Problems
# Data types
# Converting data types
# We'll be working with San Francisco bike share ride data called bike_share_rides.
# It contains information on start and end stations of each trip, the trip duration,
# and some user information.
# Before beginning to analyze any dataset, it's important to take a look at the
# different types of columns you'll be working with, which you can do using glimpse().
library(dplyr)
library(assertive)
library(stringr)
library(ggplot2)
# Connect to the database
library(DBI)
con <- dbConnect(RMySQL::MySQL(),
dbname = "tweater",
host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com",
port = 3306,
user = "student",
password = "datacamp")
# Import tweat_id column of comments where user_id is 1: elisabeth
elisabeth <- dbGetQuery(con, "SELECT tweat_id from comments where user_id = 1")
# Print elisabeth
elisabeth
# 2.2 Query tweater (2)
# Connect to the database
library(DBI)
con <- dbConnect(RMySQL::MySQL(),
dbname = "tweater",
host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com",
port = 3306,
user = "student",
password = "datacamp")
# Import post column of tweats where date is higher than '2015-09-21': latest
latest <- dbGetQuery(con, "SELECT post from tweats where date > '2015-09-21'")
# Print latest
latest
# 2.3 Query tweater (3)
# Connect to the database
library(DBI)
con <- dbConnect(RMySQL::MySQL(),
dbname = "tweater",
host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com",
port = 3306,
user = "student",
password = "datacamp")
# Create data frame specific
specific <- dbGetQuery(con, "SELECT message from comments where tweat_id = 77 and user_id >4" )
# Print specific
specific
# 2.4 Query tweater (4)
# Connect to the database
library(DBI)
con <- dbConnect(RMySQL::MySQL(),
dbname = "tweater",
host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com",
port = 3306,
user = "student",
password = "datacamp")
# Create data frame short
short <- dbGetQuery(con, "SELECT id, name from users where CHAR_LENGTH(name) < 5")
# Print short
short
# 2.5 Join the query madness!
joined <- dbGetQuery(con,"SELECT post, message
FROM tweats INNER JOIN comments on tweats.id = tweat_id
WHERE tweat_id = 77")
joined
# DBI Internals
# Sometimes you want to control the data that is fetched
# If so, you can use the DBI internal functions dbSendQuery,
# dbFetch, and dbClearResults
# 2.6 Send - Fetch - Clear
# Connect to the database
library(DBI)
con <- dbConnect(RMySQL::MySQL(),
dbname = "tweater",
host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com",
port = 3306,
user = "student",
password = "datacamp")
# Send query to the database
res <- dbSendQuery(con, "SELECT * FROM comments WHERE user_id > 4")
res
# Use dbFetch() twice
dbFetch(res, n = 2)
dbFetch(res, n = 2)
# Clear res
dbClearResult(res)
# 2.7 Be polite and ...
# Connect to the database
library(DBI)
con <- dbConnect(RMySQL::MySQL(),
dbname = "tweater",
host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com",
port = 3306,
user = "student",
password = "datacamp")
# Create the data frame  long_tweats
long_tweats <- dbGetQuery(con, "SELECT post, date from tweats where CHAR_LENGTH(post) >40")
# Print long_tweats
print(long_tweats)
# Disconnect from the database
dbDisconnect(con)
####################
# Data & Libraries #
####################
library(fst)
library(dplyr)
library(ggplot2)
library(moderndive)
library(tidyr)
library(broom)
library(tidyverse)
library(skimr)
library(lubridate)
library(tidytext)
library(timetk)
library(gt)
library(tidyverse)
library(skimr)
library(lubridate)
library(tidytext)
library(timetk)
library(gt)
# color pallete
library(tidyquant)
# modeling
library(tidymodels)
library(modeltime)
library(skimr)
library(lubridate)
library(tidytext)
install.packages("lifecycle", dependencies = FALSE)
library(tidytext)
library(timetk)
install.packages("timetk", dependencies = FALSE)
# Data Exploration
library(tidyverse)
library(skimr)
library(lubridate)
library(tidytext)
library(timetk)
library(gt)
# color pallete
library(tidyquant)
# modeling
library(tidymodels)
library(modeltime)
q()
q()
download.file(file,paste(path,file2))
package:IRkernel
install.packages("IRkernel")
install.packages("folium", dependencies = FALSE)
setwd("C:/Users/User/Documents/Andrew/Datacamp/Projects/Dr. Semmelwels and the Discovery of Handwashing")
setwd("C:/Users/User/Documents/Andrew/Datacamp/Projects/Dr. Semmelwels and the Discovery of Handwashing")
knitr::opts_chunk$set(echo = TRUE)
knitr::include_graphics("./datasets/images/ignaz_semmelweis.png")
knitr::include_graphics("./datasets/ignaz_semmelweis.png")
knitr::include_graphics("./datasets/ignaz_semmelweis_1860.png")
knitr::include_graphics("./datasets/ignaz_semmelweis_1860.jpeg")
# Load in the tidyverse package
library(tidyverse)
# Read datasets/yearly_deaths_by_clinic.csv into yearly
yearly <- read_csv("datasets/yearly_deaths_by_clinic.csv")
# Print out yearly
yearly
# Adding a new column to yearly with proportion of deaths per no. births
yearly <- yearly  %>% mutate(proportion_deaths = deaths/births)
# Print out yearly
yearly
# Setting the size of plots in this notebook
options(repr.plot.width = 7, repr.plot.height = 4)
# Plot yearly proportion of deaths at the two clinics
ggplot(yearly, aes(x = year, y = proportion_deaths, col = clinic)) +
geom_line()
# Read datasets/monthly_deaths.csv into monthly
monthly <- read_csv("datasets/monthly_deaths.csv")
# Adding a new column with proportion of deaths per no. births
monthly <- monthly  %>% mutate(proportion_deaths = deaths/births)
# Print out the first rows in monthly
head(monthly)
# Plot monthly proportion of deaths
ggplot(monthly, aes(x = date, y = proportion_deaths)) +
geom_line()
# From this date handwashing was made mandatory
handwashing_start = as.Date('1847-06-01')
# Add a TRUE/FALSE column to monthly called handwashing_started
monthly <- monthly  %>%
mutate(handwashing_started = ifelse(date >= handwashing_start, TRUE, FALSE))
# Plot monthly proportion of deaths before and after handwashing
ggplot(monthly, aes(x = date, y = proportion_deaths, col =  handwashing_started)) +
geom_line()
# Calculating the mean proportion of deaths
# before and after handwashing.
monthly_summary <- monthly  %>% group_by(handwashing_started) %>%
summarise(mean_proportion_deaths = mean(proportion_deaths))
# Printing out the summary.
monthly_summary
# Calculating a 95% Confidence intrerval using t.test
test_result <- t.test( proportion_deaths ~ handwashing_started, data = monthly)
test_result
# The data Semmelweis collected points to that:
doctors_should_wash_their_hands <- TRUE
setwd("C:/Users/User/Documents/Andrew/Datacamp/Projects/Dr. Semmelwels and the Discovery of Handwashing")
knitr::opts_chunk$set(echo = TRUE)
knitr::include_graphics("./datasets/ignaz_semmelweis_1860.jpeg")
# Load in the tidyverse package
library(tidyverse)
# Read datasets/yearly_deaths_by_clinic.csv into yearly
yearly <- read_csv("datasets/yearly_deaths_by_clinic.csv")
# Print out yearly
yearly
# Adding a new column to yearly with proportion of deaths per no. births
yearly <- yearly  %>% mutate(proportion_deaths = deaths/births)
# Print out yearly
yearly
# Setting the size of plots in this notebook
options(repr.plot.width = 7, repr.plot.height = 4)
# Plot yearly proportion of deaths at the two clinics
ggplot(yearly, aes(x = year, y = proportion_deaths, col = clinic)) +
geom_line()
# Read datasets/monthly_deaths.csv into monthly
monthly <- read_csv("datasets/monthly_deaths.csv")
# Adding a new column with proportion of deaths per no. births
monthly <- monthly  %>% mutate(proportion_deaths = deaths/births)
# Print out the first rows in monthly
head(monthly)
# Plot monthly proportion of deaths
ggplot(monthly, aes(x = date, y = proportion_deaths)) +
geom_line()
# From this date handwashing was made mandatory
handwashing_start = as.Date('1847-06-01')
# Add a TRUE/FALSE column to monthly called handwashing_started
monthly <- monthly  %>%
mutate(handwashing_started = ifelse(date >= handwashing_start, TRUE, FALSE))
# Plot monthly proportion of deaths before and after handwashing
ggplot(monthly, aes(x = date, y = proportion_deaths, col =  handwashing_started)) +
geom_line()
# Calculating the mean proportion of deaths
# before and after handwashing.
monthly_summary <- monthly  %>% group_by(handwashing_started) %>%
summarise(mean_proportion_deaths = mean(proportion_deaths))
# Printing out the summary.
monthly_summary
# Calculating a 95% Confidence intrerval using t.test
test_result <- t.test( proportion_deaths ~ handwashing_started, data = monthly)
test_result
# The data Semmelweis collected points to that:
doctors_should_wash_their_hands <- TRUE
source("~/Andrew/Datacamp/Projects/Dr. Semmelwels and the Discovery of Handwashing/Project - Dr Semmelweis Handwashing.Rmd")
